

如果用户需要同时流计算、批处理的场景下，用户需要维护两套业务代码，开发人员也要维护两套技术栈，非常不方便。 Flink 社区很早就设想过将批数据看作一个有界流数据，将批处理看作流计算的一个特例，从而实现流批统一，Flink 社区的开发人员在多轮讨论后，基本敲定了Flink 未来的技术架构



**引入依赖**

```
		<dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-table-planner_${scala.binary.version}</artifactId>
            <version>${flink.version}</version>
        </dependency>

        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-table-planner-blink_${scala.binary.version}</artifactId>
            <version>${flink.version}</version>
        </dependency>
```



### Flink-API

java代码案例1：

```java
import bean.WaterSensor;
import org.apache.flink.api.common.functions.MapFunction;
import org.apache.flink.streaming.api.TimeCharacteristic;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.functions.timestamps.AscendingTimestampExtractor;
import org.apache.flink.table.api.EnvironmentSettings;
import org.apache.flink.table.api.Table;
import org.apache.flink.table.api.java.StreamTableEnvironment;
import org.apache.flink.types.Row;

public class Flink01_SQL_TableAPI {
    public static void main(String[] args) throws Exception {
        //0.执行环境
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);

        //TODO 指定时间语义
        env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);

        SingleOutputStreamOperator<WaterSensor> sensorDS = env.readTextFile("input/Senor_data.log")
                .map(new MapFunction<String, WaterSensor>() {
                    @Override
                    public WaterSensor map(String value) throws Exception {
                        String[] datas = value.split(",");

                        return new WaterSensor(datas[0], Long.valueOf(datas[1]), Integer.valueOf(datas[2]));
                    }
                })
                .assignTimestampsAndWatermarks(
                        new AscendingTimestampExtractor<WaterSensor>() {
                            @Override
                            public long extractAscendingTimestamp(WaterSensor element) {
                                return element.getTs() * 1000L;
                            }
                        }
                );

        //TODO table API
        //1.创建表的执行环境
        EnvironmentSettings settings = EnvironmentSettings
                .newInstance()
                //使用官方的planner
                .useOldPlanner()
                //流式
                .inStreamingMode()
                .build();

        StreamTableEnvironment tableenv = StreamTableEnvironment.create(env, settings);

        //2.把DataStream 转成 table
        Table sensorTable = tableenv.fromDataStream(sensorDS, "id,ts as timestamp,vc");

        //3.使用Table API进行处理
        Table resultTable = sensorTable
                .filter("id =='senor_1'")
                .select("id,timestamp");

        //4.把table转换成 DataStream
        DataStream<Row> resultDS = tableenv.toAppendStream(resultTable, Row.class);

        resultDS.print();

        env.execute();

    }
}
```



java代码2：

```java

```

java代码案例3：

```java

```





### Flink-SQL-API



#### SQL-connect

```java
import bean.WaterSensor;
import org.apache.flink.api.common.functions.MapFunction;
import org.apache.flink.streaming.api.TimeCharacteristic;
import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.functions.timestamps.AscendingTimestampExtractor;
import org.apache.flink.table.api.DataTypes;
import org.apache.flink.table.api.EnvironmentSettings;
import org.apache.flink.table.api.Table;
import org.apache.flink.table.api.java.StreamTableEnvironment;
import org.apache.flink.table.descriptors.FileSystem;
import org.apache.flink.table.descriptors.OldCsv;
import org.apache.flink.table.descriptors.Schema;

public class Flink05_SQL_Connect {
    public static void main(String[] args) throws Exception {
        //0.执行环境
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);

        //TODO 指定时间语义
        env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);

        SingleOutputStreamOperator<WaterSensor> sensorDS = env.readTextFile("input/Senor_data.log")
                .map(new MapFunction<String, WaterSensor>() {
                    @Override
                    public WaterSensor map(String value) throws Exception {
                        String[] datas = value.split(",");

                        return new WaterSensor(datas[0], Long.valueOf(datas[1]), Integer.valueOf(datas[2]));
                    }
                })
                .assignTimestampsAndWatermarks(
                        new AscendingTimestampExtractor<WaterSensor>() {
                            @Override
                            public long extractAscendingTimestamp(WaterSensor element) {
                                return element.getTs() * 1000L;
                            }
                        }
                );

        //TODO table API
        //1.创建表的执行环境
        EnvironmentSettings settings = EnvironmentSettings
                .newInstance()
                //使用官方的planner
                .useOldPlanner()
                //流式
                .inStreamingMode()
                .build();

        StreamTableEnvironment tableenv = StreamTableEnvironment.create(env, settings);

        //2.把DataStream 转成 table
        tableenv.createTemporaryView("sensorTable",sensorDS,"id,ts,vc");


        //TODO 保存到本地文件系统

        tableenv
                .connect(new FileSystem().path("out/flink_sql.txt"))
                //指定文件类型和分隔符
                .withFormat(new OldCsv().fieldDelimiter("、"))
                //表结构
                .withSchema(
                        new Schema()
                        .field("id", DataTypes.STRING())
                         .field("ts",DataTypes.BIGINT())
                        .field("vc",DataTypes.INT())

                )
                //表名
                .createTemporaryTable("fsTable");


        //使用SQL 保存数据到文件系统
        tableenv.sqlUpdate("insert into fsTable select * from  sensorTable");

        env.execute();

    }
}
```



#### SQL-Create

```java
package chapter08;

import bean.WaterSensor;
import org.apache.flink.api.common.functions.MapFunction;
import org.apache.flink.streaming.api.TimeCharacteristic;
import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.functions.timestamps.AscendingTimestampExtractor;
import org.apache.flink.table.api.DataTypes;
import org.apache.flink.table.api.EnvironmentSettings;
import org.apache.flink.table.api.Table;
import org.apache.flink.table.api.java.StreamTableEnvironment;
import org.apache.flink.table.descriptors.FileSystem;
import org.apache.flink.table.descriptors.OldCsv;
import org.apache.flink.table.descriptors.Schema;

public class Flink06_SQL_Create {
    public static void main(String[] args) throws Exception {
        //0.执行环境
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);

        //TODO 指定时间语义
        env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);

        SingleOutputStreamOperator<WaterSensor> sensorDS = env.readTextFile("input/Senor_data.log")
                .map(new MapFunction<String, WaterSensor>() {
                    @Override
                    public WaterSensor map(String value) throws Exception {
                        String[] datas = value.split(",");

                        return new WaterSensor(datas[0], Long.valueOf(datas[1]), Integer.valueOf(datas[2]));
                    }
                })
                .assignTimestampsAndWatermarks(
                        new AscendingTimestampExtractor<WaterSensor>() {
                            @Override
                            public long extractAscendingTimestamp(WaterSensor element) {
                                return element.getTs() * 1000L;
                            }
                        }
                );

        //TODO table API
        //1.创建表的执行环境
        EnvironmentSettings settings = EnvironmentSettings
                .newInstance()
                //使用官方的planner
                .useOldPlanner()
                //流式
                .inStreamingMode()
                .build();

        StreamTableEnvironment tableenv = StreamTableEnvironment.create(env, settings);


        //TODO 获取table对象
       //方式1 ：把DataStream 转成 table
        Table table = tableenv.fromDataStream(sensorDS, "id,ts,vc");
        //方式2： 从表名获取tbale对象
        tableenv.createTemporaryView("sensorTable",sensorDS,"id,ts,vc");
        Table sensorTable = tableenv.from("sensorTable");


        //TODO 保存到本地文件系统

        tableenv
                .connect(new FileSystem().path("out/flink_sql.txt"))
                //指定文件类型和分隔符
                .withFormat(new OldCsv().fieldDelimiter("、"))
                //表结构
                .withSchema(
                        new Schema()
                        .field("id", DataTypes.STRING())
                         .field("ts",DataTypes.BIGINT())
                        .field("vc",DataTypes.INT())

                )
                //表名
                .createTemporaryTable("fsTable");


        //使用SQL 保存数据到文件系统
        tableenv.sqlUpdate("insert into fsTable select * from  sensorTable");

        env.execute();

    }
}

```



#### SQL-Operator

```java
package chapter08;

import bean.WaterSensor;
import org.apache.flink.api.common.functions.MapFunction;
import org.apache.flink.streaming.api.TimeCharacteristic;
import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.functions.timestamps.AscendingTimestampExtractor;
import org.apache.flink.table.api.EnvironmentSettings;
import org.apache.flink.table.api.Table;
import org.apache.flink.table.api.java.StreamTableEnvironment;
import org.apache.flink.types.Row;

public class Flink08_SQL_Operator {
    public static void main(String[] args) throws Exception {
        //0.执行环境
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);

        //TODO 指定时间语义
        env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);

        SingleOutputStreamOperator<WaterSensor> sensorDS = env.readTextFile("input/Senor_data.log")
                .map(new MapFunction<String, WaterSensor>() {
                    @Override
                    public WaterSensor map(String value) throws Exception {
                        String[] datas = value.split(",");

                        return new WaterSensor(datas[0], Long.valueOf(datas[1]), Integer.valueOf(datas[2]));
                    }
                })
                .assignTimestampsAndWatermarks(
                        new AscendingTimestampExtractor<WaterSensor>() {
                            @Override
                            public long extractAscendingTimestamp(WaterSensor element) {
                                return element.getTs() * 1000L;
                            }
                        }
                );

        SingleOutputStreamOperator<WaterSensor> sensorDS1 = env.readTextFile("input/Senor_data-cep.log")
                .map(new MapFunction<String, WaterSensor>() {
                    @Override
                    public WaterSensor map(String value) throws Exception {
                        String[] datas = value.split(",");

                        return new WaterSensor(datas[0], Long.valueOf(datas[1]), Integer.valueOf(datas[2]));
                    }
                })
                .assignTimestampsAndWatermarks(
                        new AscendingTimestampExtractor<WaterSensor>() {
                            @Override
                            public long extractAscendingTimestamp(WaterSensor element) {
                                return element.getTs() * 1000L;
                            }
                        }
                );

        //TODO 使用SQL 操作table
        //1.创建表的执行环境
        EnvironmentSettings settings = EnvironmentSettings
                .newInstance()
                //使用官方的planner
                .useOldPlanner()
                //流式
                .inStreamingMode()
                .build();

        StreamTableEnvironment tableenv = StreamTableEnvironment.create(env, settings);

        //2.把DataStream 转成 table
        Table sensorTable = tableenv.fromDataStream(sensorDS, "id,ts as timestamp,vc");

        //TODO 使用 SQL 进行处理

        tableenv.createTemporaryView("sensorTable",sensorDS,"id,ts,vc");
        tableenv.createTemporaryView("sensorTable1",sensorDS1,"id,ts,vc");

        //table
        Table resultTable = tableenv
                //查询
//                .sqlQuery("select * from sensorTable" );
                //连接
                .sqlQuery("select * from sensorTable as a left join sensorTable1 as b on a.id=b.id" );


        tableenv
                .toRetractStream(resultTable, Row.class)
                .print();


        env.execute();

    }
}

```



#### SQL-GroupWindow

```java
package chapter08;

import bean.WaterSensor;
import org.apache.flink.api.common.functions.MapFunction;
import org.apache.flink.streaming.api.TimeCharacteristic;
import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.functions.timestamps.AscendingTimestampExtractor;
import org.apache.flink.table.api.EnvironmentSettings;
import org.apache.flink.table.api.Slide;
import org.apache.flink.table.api.Table;
import org.apache.flink.table.api.Tumble;
import org.apache.flink.table.api.java.StreamTableEnvironment;
import org.apache.flink.types.Row;

public class Flink09_SQL_GroupWindow {
    public static void main(String[] args) throws Exception {
        //0.执行环境
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);

        //TODO 指定时间语义
        env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);

        SingleOutputStreamOperator<WaterSensor> sensorDS = env.readTextFile("input/Senor_data.log")
                .map(new MapFunction<String, WaterSensor>() {
                    @Override
                    public WaterSensor map(String value) throws Exception {
                        String[] datas = value.split(",");

                        return new WaterSensor(datas[0], Long.valueOf(datas[1]), Integer.valueOf(datas[2]));
                    }
                })
                .assignTimestampsAndWatermarks(
                        new AscendingTimestampExtractor<WaterSensor>() {
                            @Override
                            public long extractAscendingTimestamp(WaterSensor element) {
                                return element.getTs() * 1000L;
                            }
                        }
                );



        //TODO 使用SQL 操作table
        //1.创建表的执行环境
        EnvironmentSettings settings = EnvironmentSettings
                .newInstance()
                //使用官方的planner
                .useOldPlanner()
                //流式
                .inStreamingMode()
                .build();

        StreamTableEnvironment tableenv = StreamTableEnvironment.create(env, settings);


        //TODO 使用 SQL 进行处理

        tableenv.createTemporaryView("sensorTable",sensorDS,"id,ts.rowtime as rt,vc");

        Table sensorTable = tableenv.from("sensorTable");

        //指定窗口类型：Tumber、Slide 、Session
        //必须把窗口放在分组字段里
        //可以用 窗口.start 窗口.end 获取窗口的开始和结束时间
        Table resultTable = sensorTable
//                .window(Tumble.over("5.seconds").on("rt").as("w"))
                .window(Slide.over("5.seconds").every("2.seconds").on("rt").as("w"))
                .groupBy("id,w")
                .select("id,count(id),w.start,w.end");

        tableenv.toRetractStream(resultTable,Row.class).print();



        env.execute();

    }
}

```



#### SQL-OverWindow

```java
package chapter08;

import bean.WaterSensor;
import org.apache.flink.api.common.functions.MapFunction;
import org.apache.flink.streaming.api.TimeCharacteristic;
import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.functions.timestamps.AscendingTimestampExtractor;
import org.apache.flink.table.api.EnvironmentSettings;
import org.apache.flink.table.api.Over;
import org.apache.flink.table.api.Slide;
import org.apache.flink.table.api.Table;
import org.apache.flink.table.api.java.StreamTableEnvironment;
import org.apache.flink.types.Row;

public class Flink10_SQL_OverWindow {
    public static void main(String[] args) throws Exception {
        //0.执行环境
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);

        //TODO 指定时间语义
        env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);

        SingleOutputStreamOperator<WaterSensor> sensorDS = env.readTextFile("input/Senor_data.log")
                .map(new MapFunction<String, WaterSensor>() {
                    @Override
                    public WaterSensor map(String value) throws Exception {
                        String[] datas = value.split(",");

                        return new WaterSensor(datas[0], Long.valueOf(datas[1]), Integer.valueOf(datas[2]));
                    }
                })
                .assignTimestampsAndWatermarks(
                        new AscendingTimestampExtractor<WaterSensor>() {
                            @Override
                            public long extractAscendingTimestamp(WaterSensor element) {
                                return element.getTs() * 1000L;
                            }
                        }
                );



        //TODO 使用SQL 操作table
        //1.创建表的执行环境
        EnvironmentSettings settings = EnvironmentSettings
                .newInstance()
                //使用官方的planner
                .useOldPlanner()
                //流式
                .inStreamingMode()
                .build();

        StreamTableEnvironment tableenv = StreamTableEnvironment.create(env, settings);


        //TODO 使用 SQL 进行处理

        tableenv.createTemporaryView("sensorTable",sensorDS,"id,ts.rowtime as rt,vc");

        Table sensorTable = tableenv.from("sensorTable");
        //tbale API
//        Table resultTable = sensorTable
//                .window(
//                        Over
//                                .partitionBy("id")
//                                .orderBy("rt")
//                                .preceding("UNCOUNDED_RANGE")
//                                .following("CURRENT_RANGE")
//                                .as("ow")
//                )
//                .select("id,count(id) over ow");

        //SQL
        Table resultTable = tableenv.sqlQuery("select id," +
                "count(id) over(partition by id order by rt ROWS BETWEEN 2 PRECEDING AND CURRENT ROW ) " +
                "FROM sensorTable");


        tableenv.toRetractStream(resultTable,Row.class).print();


        env.execute();

    }
}

```





#### 热门商品Top N

```java
package chapter08;


import bean.UserBehavior;
import org.apache.flink.api.common.functions.MapFunction;
import org.apache.flink.streaming.api.TimeCharacteristic;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.functions.timestamps.AscendingTimestampExtractor;
import org.apache.flink.table.api.EnvironmentSettings;
import org.apache.flink.table.api.Slide;
import org.apache.flink.table.api.Table;
import org.apache.flink.table.api.java.StreamTableEnvironment;
import org.apache.flink.types.Row;


public class Flink11_SQL_HotItemsAnalysis {
    public static void main(String[] args) throws Exception {

        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);
        env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);

        //1.读取数据
        SingleOutputStreamOperator<UserBehavior> userBehaviorDS = env.readTextFile("input/UserBehavior.csv")
                .map(new MapFunction<String, UserBehavior>() {
                    @Override
                    public UserBehavior map(String value) throws Exception {
                        String[] datas = value.split(",");
                        return new UserBehavior(
                                Long.valueOf(datas[0]),
                                Long.valueOf(datas[1]),
                                Integer.valueOf(datas[2]),
                                datas[3],
                                Long.valueOf(datas[4]));
                    }
                })
                //设置watermark
                .assignTimestampsAndWatermarks(
                        new AscendingTimestampExtractor<UserBehavior>() {
                            @Override
                            public long extractAscendingTimestamp(UserBehavior element) {
                                return element.getTimestamp() * 1000L;
                            }
                        }
                );

        //TODO 使用table API 和SQL来实现Top N => 需要使用blink
        //创建表的执行环境
        EnvironmentSettings settings = EnvironmentSettings
                .newInstance()
                .useBlinkPlanner()
                .inStreamingMode()
                .build();

        StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env, settings);

        //从流中获取table对象
        Table table = tableEnv.fromDataStream(userBehaviorDS, "itemId,behavior,timestamp");

        //使用table API
        Table aggTable = table
                .filter("behavior == 'pv' ")
                .window(Slide.over("1.hours").every("5.minutes").on("timestamp").as("w"))
                .groupBy("itemId,w")
                .select("itemId,count(itemId) as cnt,w.end as windowEnd");

        //需要先转成数据流 -> 防止自动转换类型 ，出现匹配不上的情况
        DataStream<Row> aggDS = tableEnv.toAppendStream(aggTable, Row.class);


        //使用 SQL 实现TOP N 的排序
//        tableEnv.createTemporaryView("aggTable",aggTable);
        tableEnv.createTemporaryView("aggTable",aggDS,"itemId,cnt,windowEnd");

        Table top3Table = tableEnv.sqlQuery("select" +
                "* " +
                "from " +
                "(select" +
                "*," +
                "row_number() over(partiton by windowEnd order by cnt desc) as randnum" +
                "from aggTable)" +
                "where randnum<=3");

        tableEnv.toRetractStream(top3Table, Row.class).print();


        env.execute();

    }


}

```



#### 更新模式

- 对于流式查询，需要声明如何在表和外部连接器之间执行转换
- 与外部系统交换的消息类型，由更新模式（Uadate Mode）指定
- 追加（Append）模式
  - 表只做插入操作，和外部连接器只交换插入（Insert）消息
- 撤回（Retract）模式
  - 表和外部连接器交换添加（Add）和撤回（Retract）消息
  - 插入操作（Insert）编码为Add消息；删除（Delete）编码为Retract消息；**更新（Update）编码为上一条的Retract和下一条的Add消息**
- 更新插入（Upsert）模式
  - 更新和插入都被编码为Upsert消息；删除编码为Delete消息