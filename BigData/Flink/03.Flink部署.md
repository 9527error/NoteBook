

### Flink部署



#### Local模式

- 将flink-1.10.0-bin-scala_2.11.tgz文件上传到Linux并解压缩

  ```
  tar -zxvf flink-1.10.0-bin-scala_2.11.tgz -C /opt/module
  mv flink-1.10.0 flink
  ```

- 进入解压缩路径，启动local环境

  ```
  bin/start-scala-shell.sh local
  ```

- 启动成功后，输入网址进入web ui

  ```
  http://虚拟机地址:8081
  ```



#### Standlone模式

- 解压缩文件

  ```
  tar -zxvf flink-1.10.0-bin-scala_2.11.tgz -C /opt/module
  mv flink-1.10.0 flink
  ```

- 修改配置文件

  - 修改 conf/flink-conf.yaml 文件

    ```
    jobmanager.rpc.address: hadoop102
    ```

  - 修改 conf/slaves文件

    ```
    Hadoop103
    Hadoop104
    ```

  - 分发Flink到其他节点

    ```
    xsync flink
    ```

  -  执行启动命令     

    ```
    bin/start-cluster.sh
    ```

  - 访问WEB UI

    ```
    http://hadoop102:8081
    http://192.168.32.101:8081
    ```

- HA-高可用

  - 修改conf/flink-conf.yaml配置文件

    配置参数中冒号后面的参数值都需要增加空格

    ```
    # Line79
    high-availability: zookeeper
    
    # Line88
    high-availability.storageDir: hdfs://hadoop102:9000/flink/ha/
    
    # Line94
    high-availability.zookeeper.quorum: hadoop102:2282,hadoop103:2282,hadoop104:2282
    ```

  -  修改conf/master配置文件

    ```
    Hadoop102:8081
    Hadoop103:8081
    ```

  - 修改zoo.cfg配置文件

    ```
    #Line 32 防止和外部ZK冲突
    clientPort=2282
    #Line 35
    server.88=hadoop102:2888:3888
    server.89=hadoop103:2888:3888
    server.90=hadoop104:2888:3888
    ```

  - 分发配置文件

    ```
    xsync conf
    ```

  -  启动HDFS集群

    引入hadoop关联jar包：flink-shaded-hadoop-2-uber-2.7.5-7.0.jar

    分发Jar包

  - 启动Flink Zookeeper集群

  - 启动Flink-HA集群

  - 访问地址

    ```
    http://hadoop102:8081
    http://hadoop103:8081
    ```



#### windows模式

- 安装jdk，配置java环境

- 将文件flink-1.10.0-bin-scala_2.11.tgz解压缩到无中文无空格的路径中

- 修改conf/flink-conf.yaml文件，添加配置

  ```
  taskmanager.cpu.cores: 1.8
  taskmanager.memory.task.heap.size: 2048m
  taskmanager.memory.managed.size: 2048m
  taskmanager.memory.network.fraction: 0.1
  taskmanager.memory.network.min: 64mb
  taskmanager.memory.network.max: 64mb
  ```

- 执行脚本：bin/start-cluster.bat，启动两个进程

- 打开ui界面：http://localhost:8081/.



#### Yarn模式

​	    Flink提供了两种在yarn上运行的模式，分别为Session-Cluster和Per-Job-Cluster模式。

##### Per-Job-Cluster模式

- 一个job对应一个yarn-session集群，单独申请资源，作业之间互不影响
- 独享Dispatcher和ResourceManager
- 按需接受资源申请，**适合规模大长时间运行的作业**
- 每次任务提交都会创建一个Flink集群，任务执行完集群也会消失



##### Session-Cluster模式

- 在Yarn中初始化一个flink集群，开辟指定资源，之后提交任务都是在这个集群提交
- 这个集群常驻在yarn集群，除非手动停止
- 资源永远保持不变
- 如果资源满了，下一个作业就无法提交，只能等到 yarn 中的其中一个作业执行完成后，释放了资源，下个作业才会正常提交 
- 所有作业共享 Dispatcher 和 ResourceManager；共享资源；**适合规模小执行时间短的作业。**



#### Flink调度原理

```shell

```

1. Flink 中每一个 TaskManager 都是一个JVM进程，它可能会在独立的线程上执行一个或多个 subtask
2. 为了控制一个 TaskManager 能接收多少个 task， TaskManager 通过 task slot 来进行控制（一个 TaskManager 至少有一个 slot）
3. 每个task slot表示TaskManager拥有资源的一个固定大小的子集。假如一个TaskManager有三个slot，那么它会将其管理的内存分成三份给各个slot(注：这里不会涉及CPU的隔离，slot仅仅用来隔离task的受管理内存)
4. 可以通过调整task slot的数量去自定义subtask之间的隔离方式。如一个TaskManager一个slot时，那么每个task group运行在独立的JVM中。而**当一个TaskManager多个slot时，多个subtask可以共同享有一个JVM,而在同一个JVM进程中的task将共享TCP连接和心跳消息，也可能共享数据集和数据结构，从而减少每个task的负载**。



Yarn会按需动态分配TaskManager和slot

```
bin/yarn-session.sh -d -n 2 -s 2 -jm 1024 -tm 1024 -nm test
```

| 参数            | 说明                                                         |
| --------------- | ------------------------------------------------------------ |
| -n(--container) | TaskManager的数量                                            |
| -s(--slots)     | 每个TaskManager的slot数量，默认一个slot一个core，默认每个taskmanager的slot的个数为1，有时可以多一些taskmanager，做冗余 |
| -jm             | JobManager的内存（单位MB)                                    |
| -tm             | 每个Taskmanager的内存（单位MB)                               |
| -nm             | yarn 的appName(现在yarn的ui上的名字)                         |
| -d              | 后台执行,需要放在前面，否则不生效                            |

​	