### Spark框架



#### 什么是Spark

Spark 是一种基于内存的快速、通用、可扩展的大数据分析计算引擎。



#### Sprak  VS Hadoop

 

- Spark 是一个分布式数据快速分析项目。它的**核心技术是弹性分布式数据集**（Resilient  

Distributed Datasets），提供了比 MapReduce 丰富的模型，可以快速在内存中对数据集 

进行多次迭代，来支持复杂的数据挖掘算法和图形计算算法。 

- **Spark 和Hadoop 的根本差异是多个作业之间的数据通信问题 : Spark 多个作业之间数据** 

**通信是基于内存，而 Hadoop 是基于磁盘。** 

-  Spark Task 的启动时间快。Spark 采用 fork 线程的方式，而 Hadoop 采用创建新的进程 

的方式。 

- Spark 只有在 shuffle 的时候将数据写入磁盘，而 Hadoop 中多个 MR 作业之间的数据交 

互都要依赖于磁盘交互 

- Spark 的缓存机制比 HDFS 的缓存机制高效。 

经过上面的比较，我们可以看出在绝大多数的数据计算场景中，Spark 确实会比 MapReduce 

更有优势。但是 Spark 是基于内存的，所以在实际的生产环境中，**由于内存的限制，可能会** 

**由于内存资源不够导致 Job 执行失败，此时，MapReduce 其实是一个更好的选择，所以 Spark** 

**并不能完全替代 MR**。





#### Spark的核心模块

➢ **Spark Core** 

Spark Core 中提供了 Spark 最基础与最核心的功能，Spark 其他的功能如：Spark SQL， 

Spark Streaming，GraphX, MLlib 都是在 Spark Core 的基础上进行扩展的 

➢ **Spark SQL** 

Spark SQL 是 Spark 用来操作结构化数据的组件。通过 Spark SQL，用户可以使用 SQL 

或者 Apache Hive 版本的 SQL 方言（HQL）来查询数据。 

➢ **Spark Streaming** 

Spark Streaming 是 Spark 平台上针对实时数据进行流式计算的组件，提供了丰富的处理 

数据流的 API。 

➢ **Spark MLlib** 

MLlib 是 Spark 提供的一个机器学习算法库。MLlib 不仅提供了模型评估、数据导入等 

额外的功能，还提供了一些更底层的机器学习原语。 

➢ **Spark GraphX** 

GraphX 是 Spark 面向图计算提供的框架与算法库。 





#### Spark WordCount

```scala
// 创建 Spark 运行配置对象
val sparkConf = new SparkConf().setMaster("local[*]").setAppName("WordCount")
// 创建 Spark 上下文环境对象（连接对象）
val sc : SparkContext = new SparkContext(sparkConf)
// 读取文件数据
val fileRDD: RDD[String] = sc.textFile("input/word.txt")
// 将文件中的数据进行分词
val wordRDD: RDD[String] = fileRDD.flatMap( _.split(" ") )
// 转换数据结构 word => (word, 1)
val word2OneRDD: RDD[(String, Int)] = wordRDD.map((_,1))
// 将转换结构后的数据按照相同的单词进行分组聚合
val word2CountRDD: RDD[(String, Int)] = word2OneRDD.reduceByKey(_+_)
// 将数据聚合结果采集到内存中
val word2Count: Array[(String, Int)] = word2CountRDD.collect()
// 打印结果
word2Count.foreach(println)
//关闭 Spark 连接
sc.stop()
```

执行过程中，会产生大量的执行日志，如果为了能够更好的查看程序的执行结果，可以在项 

目的 **resources 目录中创建 log4j.properties 文件**，并添加日志配置信息： 

```
log4j.rootCategory=ERROR, console
log4j.appender.console=org.apache.log4j.ConsoleAppender
log4j.appender.console.target=System.err
log4j.appender.console.layout=org.apache.log4j.PatternLayout
log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd 
HH:mm:ss} %p %c{1}: %m%n
# Set the default spark-shell log level to ERROR. When running the spark-shell, 
the
# log level for this class is used to overwrite the root logger's log level, so 
that
# the user can have different defaults for the shell and regular Spark apps.
log4j.logger.org.apache.spark.repl.Main=ERROR
# Settings to quiet third party logs that are too verbose
log4j.logger.org.spark_project.jetty=ERROR
log4j.logger.org.spark_project.jetty.util.component.AbstractLifeCycle=ERROR
log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=ERROR
log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=ERROR
log4j.logger.org.apache.parquet=ERROR
log4j.logger.parquet=ERROR
# SPARK-9183: Settings to avoid annoying messages when looking up nonexistent 
UDFs in SparkSQL with Hive support
log4j.logger.org.apache.hadoop.hive.metastore.RetryingHMSHandler=FATAL
log4j.logger.org.apache.hadoop.hive.ql.exec.FunctionRegistry=ERROR
```



#### Sprak运行环境

##### Local模式

一般用于教学，调试，演示等

1.将 spark-3.0.0-bin-hadoop3.2.tgz 文件上传到 Linux 并解压缩，放置在指定位置，路径中 

不要包含中文或空格

```shell
tar -zxvf spark-3.0.0-bin-hadoop3.2.tgz -C /opt/module
cd /opt/module 
mv spark-3.0.0-bin-hadoop3.2 spark-local
```

2.启动local环境，进入解压缩路径，执行指令

```shell
bin/spark-shell
```

3.启动成功，输入网址进行web UI页面进行监控

```
http://虚拟机地址:4040
```

4.命令行测试

- 在data目录下创建word.txt文件，进行wordcount测试

- 代码测试

  ```shell
  sc.textFile("data/word.txt").flatMap(_.split("
  ")).map((_,1)).reduceByKey(_+_).collect
  ```

- 退出本地模式(两种方式)

  ```
  1.ctrl +c
  2.命令行输入：:quit
  ```

5.提交应用

```
bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master local[2] \
./examples/jars/spark-examples_2.12-3.0.0.jar \
10
```

1) --class 表示要执行程序的主类，此处可以更换为咱们自己写的应用程序 

2) --master local[2] 部署模式，默认为本地模式，数字表示分配的虚拟 CPU 核数量 

3) spark-examples_2.12-3.0.0.jar 运行的应用类所在的 jar 包，实际使用时，可以设定为咱 

们自己打的 jar 包 

4) 数字 10 表示程序的入口参数，用于设定当前应用的任务数量



##### Standalone模式

只使用 Spark 自身节点运行的集群模式，也就是我们所谓的独立部署（Standalone）模式

集群规划：

|       | Linux1      | Linux2 | Linux3 |
| ----- | ----------- | ------ | ------ |
| spark | Work Master | Work   | Work   |

1.解压缩文件

```
tar -zxvf spark-3.0.0-bin-hadoop3.2.tgz -C /opt/module
cd /opt/module 
mv spark-3.0.0-bin-hadoop3.2 spark-standalone
```

2.修改配置文件

- 进入解压缩后路径的 conf 目录，修改 slaves.template 文件名为 slaves

  ```
  mv slaves.template slaves
  ```

- 修改 slaves 文件，添加 work 节点

  ```
  linux1
  linux2
  linux3
  ```

- 修改 spark-env.sh.template 文件名为 spark-env.sh

  ```
  mv spark-env.sh.template spark-env.sh
  ```

-  修改 spark-env.sh 文件，添加 JAVA_HOME 环境变量和集群对应的 master 节点

  ```
  export JAVA_HOME=/opt/module/jdk1.8.0_144
  SPARK_MASTER_HOST=linux1
  SPARK_MASTER_PORT=7077
  ```

  注意：7077 端口，相当于 hadoop3 内部通信的 8020 端口，此处的端口需要确认自己的 Hadoop 

  配置

- 分发 spark-standalone 目录

  ```
  xsync spark-standalone
  ```

3.启动集群

- 执行命令

  ```
  sbin/start-all.sh
  ```

- 查看服务器运行进程

- 查看Web UI界面

  ```
  http://linux1:8080
  ```



  4.提交应用

  ```
  bin/spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master spark://linux1:7077 \
  ./examples/jars/spark-examples_2.12-3.0.0.jar \
  10
  ```

  1) --class 表示要执行程序的主类 

  2) --master spark://linux1:7077 独立部署模式，连接到 Spark 集群 

  3) spark-examples_2.12-3.0.0.jar 运行类所在的 jar 包 

  4) 数字 10 表示程序的入口参数，用于设定当前应用的任务数量



5.配置历史服务

由于 spark-shell 停止掉后，集群监控 linux1:4040 页面就看不到历史任务的运行情况，所以 

开发时都配置历史服务器记录任务运行情况

- 修改 spark-defaults.conf.template 文件名为 spark-defaults.conf

  ```
  mv spark-defaults.conf.template spark-defaults.conf
  ```

- 修改 spark-default.conf 文件，配置日志存储路径

  ```
  spark.eventLog.enabled true
  spark.eventLog.dir hdfs://linux1:8020/directory
  ```

- 注意：需要启动 hadoop 集群，HDFS 上的 directory 目录需要提前存在

  ```
  sbin/start-dfs.sh
  hadoop fs -mkdir /directory
  ```

- 修改 spark-env.sh 文件, 添加日志配置

  ```
  export SPARK_HISTORY_OPTS="
  -Dspark.history.ui.port=18080 
  -Dspark.history.fs.logDirectory=hdfs://linux1:8020/directory 
  -Dspark.history.retainedApplications=30"
  ```

  参数 1 含义：WEB UI 访问的端口号为 18080 

  参数 2 含义：指定历史服务器日志存储路径 

  参数 3 含义：指定保存 Application 历史记录的个数，如果超过这个值，旧的应用程序 

  信息将被删除，这个是内存中的应用数，而不是页面上显示的应用数。 

- 分发配置文件

  ```
  xsync conf
  ```

- 重新启动集群和历史服务

  ```
  sbin/start-all.sh
  sbin/start-history-server.sh
  ```

- 重新执行任务

  ```
  bin/spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master spark://linux1:7077 \
  ./examples/jars/spark-examples_2.12-3.0.0.jar \
  10
  ```

- 查看历史服务

  ```
  http://linux1:18080
  ```



##### 高可用(HA)

高可用是因为当前集群中的 Master 节点只有一个，所以会存在单点故障问题。所以 为了解决单点故障问题，需要在集群中配置多个 Master 节点，一旦处于活动状态的 Master 发生故障时，由备用 Master 提供服务，保证作业可以继续执行。

集群规划：

|       | Linux1                     | Linux2                     | Linux3            |
| ----- | -------------------------- | -------------------------- | ----------------- |
| Spark | Master 、Zookeeper、Worker | Master 、Zookeeper、Worker | Zookeeper、Worker |

- 停止集群

  ```
  sbin/stop-all.sh
  ```

- 启动Zookeeper

  ```
  xstart zk
  ```

- 修改 spark-env.sh 文件添加如下配置

  ```
  注释如下内容：
  #SPARK_MASTER_HOST=linux1
  #SPARK_MASTER_PORT=7077
  添加如下内容:
  #Master 监控页面默认访问端口为 8080，但是可能会和 Zookeeper 冲突，所以改成 8989，也可以自
  定义，访问 UI 监控页面时请注意
  SPARK_MASTER_WEBUI_PORT=8989
  export SPARK_DAEMON_JAVA_OPTS="
  -Dspark.deploy.recoveryMode=ZOOKEEPER 
  -Dspark.deploy.zookeeper.url=linux1,linux2,linux3
  -Dspark.deploy.zookeeper.dir=/spark"
  ```

- 分发配置文件

  ```
  xsync conf/
  ```

- 启动集群

  ```
  sbin/start-all.sh
  ```

- 启动 linux2 的单独 Master 节点，此时 linux2 节点 Master 状态处于备用状态

- 提交应用到高可用集群

  ```
  bin/spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master spark://linux1:7077,linux2:7077 \
  ./examples/jars/spark-examples_2.12-3.0.0.jar \
  10
  ```

- 停止 linux1 的 Master 资源监控进程

  ```
  kill -9 6703
  ```

- 查看 linux2 的 Master 资源监控 Web UI，稍等一段时间后，linux2 节点的 Master 状态提升为活动状态



##### Yarn模式

Spark主要是计算框架，而不是资源调度框架，所以本身提供的资源调度并不是它的强项，所以还是和其他专业的资源调度框架集成会更靠谱一些。

1.解压缩文件

```
tar -zxvf spark-3.0.0-bin-hadoop3.2.tgz -C /opt/module
cd /opt/module 
mv spark-3.0.0-bin-hadoop3.2 spark-yarn
```

2.修改配置文件

- 修改 hadoop 配置文件/opt/module/hadoop/etc/hadoop/yarn-site.xml, 并分发

  ```
  <!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认
  是 true -->
  <property>
   <name>yarn.nodemanager.pmem-check-enabled</name>
   <value>false</value>
  </property>
  <!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认
  是 true -->
  <property>
   <name>yarn.nodemanager.vmem-check-enabled</name>
   <value>false</value>
  </property>
  ```

- 修改 conf/spark-env.sh，添加 JAVA_HOME 和 YARN_CONF_DIR 配置

  ```
  mv spark-env.sh.template spark-env.sh
  。。。
  export JAVA_HOME=/opt/module/jdk1.8.0_144
  YARN_CONF_DIR=/opt/module/hadoop/etc/hadoop
  ```



3.启动HDFS及YARN集群

4.提交应用

```

```

 点击http://linux2:8088 页面，点击 History



5.配置历史服务器

- 修改 spark-defaults.conf.template 文件名为 spark-defaults.conf

  ```
  
  ```

- 修改 spark-default.conf 文件，配置日志存储路径

  ```
  
  ```

- 注意：需要启动 hadoop 集群，HDFS 上的目录需要提前存在

  ```
  
  ```

- 修改 spark-env.sh 文件, 添加日志配置

  ```
  
  ```

  参数 1 含义：WEB UI 访问的端口号为 18080 

  参数 2 含义：指定历史服务器日志存储路径 

  参数 3 含义：指定保存 Application 历史记录的个数，如果超过这个值，旧的应用程序 

  信息将被删除，这个是内存中的应用数，而不是页面上显示的应用数。 

- 修改 spark-defaults.conf

  ```
  spark.yarn.historyServer.address=linux1:18080
  spark.history.ui.port=18080
  ```

- 启动历史服务

  ```
  sbin/start-history-server.sh
  ```

- 重新提交应用

  ```
  bin/spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master yarn \
  --deploy-mode client \
  ./examples/jars/spark-examples_2.12-3.0.0.jar \
  10
  ```

- Web 页面查看日志：http://linux2:8088



##### Windows模式

自己学习时，每次都需要启动虚拟机，启动集群，这是一个比较繁琐的过程，并且会占大量的系统资源，导致系统执行变慢，不仅仅影响学习效果，也影响学习进度，Spark 非常暖心地提供了可以在 windows 系统下启动本地集群的方式

1.解压缩文件

将文件 spark-3.0.0-bin-hadoop3.2.tgz 解压缩到无中文无空格的路径中

2.启动本地环境

- 执行解压缩文件路径下 bin 目录中的 spark-shell.cmd 文件，启动 Spark 本地环境

- 在 bin 目录中创建 input 目录，并添加 word.txt 文件, 在命令行中输入脚本代码

  ```shell
  sc.textFile("input/word.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).collect
  ```

  sc.textFile("input/word.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).collect

3.在 DOS 命令行窗口中执行提交指令

```
spark-submit --class org.apache.spark.examples.SparkPi --master 
local[2] ../examples/jars/spark-examples_2.12-3.0.0.jar 10
```

spark-submit --class org.apache.spark.examples.SparkPi --master local[2] ../examples/jars/spark-examples_2.12-3.0.0.jar 10



**端口号**

➢ Spark 查看当前 Spark-shell 运行任务情况端口号：4040（计算） 

➢ Spark Master 内部通信服务端口号：7077 

➢ Standalone 模式下，Spark Master Web 端口号：8080（资源） 

➢ Spark 历史服务器端口号：18080 

➢ Hadoop YARN 任务运行情况查看端口号：8088 





#### Spark运行架构

##### Driver

Spark 驱动器节点，**用于执行 Spark 任务中的 main 方法，负责实际代码的执行工作。** 

Driver 在 Spark 作业执行时主要负责： 

➢ 将用户程序转化为作业（job） 

➢ 在 Executor 之间调度任务(task) 

➢ 跟踪 Executor 的执行情况 

➢ 通过 UI 展示查询运行情况 

实际上，我们无法准确地描述 Driver 的定义，因为在整个的编程过程中没有看到任何有关 Driver 的字眼。所以简单理解，所谓的 Driver 就是驱使整个应用运行起来的程序，也称之为 Driver 类。 



##### Executor

Spark Executor 是集群中工作节点（Worker）中的一个 JVM 进程，负责在 Spark 作业 中运行具体任（Task），**任务彼此之间相互独立**。Spark 应用启动时，Executor 节点被同时启动，并且始终伴随着整个 Spark 应用的生命周期而存在**。如果有 Executor 节点发生了 故障或崩溃，Spark 应用也可以继续执行，会将出错节点上的任务调度到其他 Executor 节点上继续运行。** 

Executor 有两个核心功能： 

➢ 负责运行组成 Spark 应用的任务，并将结果返回给驱动器进程 

➢ 它们通过自身的块管理器（Block Manager）为用户程序中要求缓存的 RDD **提供内存式存储**。RDD 是直接缓存在 Executor 进程内的，因此任务可以在运行时充分利用缓存数据加速运算。



##### Master & Worker

Spark 集群的独立部署环境中，不需要依赖其他的资源调度框架，**自身就实现了资源调度的功能**，所以环境中还有其他两个核心组件：Master 和 Worker，这里的 **Master 是一个进程，主要负责资源的调度和分配，**并进行集群的监控等职责，类似于 Yarn 环境中的 RM, 而 Worker 呢，也是进程，一个 Worker 运行在集群中的一台服务器上，由 Master 分配资源对 **数据进行并行的处理和计算，**类似于 Yarn 环境中 NM。



##### Application Master

Hadoop 用户向 YARN 集群提交应用程序时,提交程序中应该包含 ApplicationMaster，用 于向资源调度器申请执行任务的资源容器 Container，运行用户自己的程序任务 job，监控整 个任务的执行，跟踪整个任务的状态，处理任务失败等异常情况。 说的简单点就是，**ResourceManager（资源）和 Driver（计算）之间的解耦合靠的就是** 

**ApplicationMaster。** 



##### Executor 和 core

Spark Executor 是集群中运行在工作节点（Worker）中的一个 JVM 进程，是整个集群中 的专门用于计算的节点。在提交应用中，可以提供参数指定计算节点的个数，以及对应的资 源。这里的资源一般指的是工作节点 Executor 的内存大小和使用的虚拟 CPU 核（Core）数量。

| 名称              | 说明                           |
| ----------------- | ------------------------------ |
| --nums-executors  | 配置executor的数量             |
| --executor-memory | 配置每个executor的内存大小     |
| --executor-cores  | 配置每个executor的CPU core数量 |



##### 并行度(Parallelism)

在分布式计算框架中一般都是多个任务同时执行，由于任务分布在不同的计算节点进行 计算，所以能够真正地实现多任务并行执行，记住，这里是并行，而不是并发。**这里我们将 整个集群并行执行任务的数量称之为并行度。**那么一个作业到底并行度是多少呢？这个取决 于框架的默认配置。应用程序也可以在运行过程中动态修改。 



##### 有向无环图(DAG)

是由 Spark 程序直接映射成的数据 流的高级抽象模型。简单理解就是**将整个程序计算的执行过程用图形表示出来,**这样更直观， 更便于理解，可以用于表示程序的拓扑结构

DAG（Directed Acyclic Graph）有向无环图是由点和线组成的拓扑图形，该图形具有方向，不会闭环。



#### 提交流程

就是我们开发人员根据需求写的应用程序通过 Spark 客户端提交给 Spark 运行环境执行计算的流程。在不同的部署环境中，这个提交过程基本相同，但是又 有细微的区别，这里以Yarn举例

```
任务提交->Driver运行->注册应用程序到集群管理器->Executor启动->反向注册到Driver

Driver资源满足后->执行main函数->main懒执行到action算子->算子触发job到stage划分

->创建taskSet->将task分发给指定的Executor
```

![1569211205051](C:\Users\87603\AppData\Roaming\Typora\typora-user-images\1569211205051.png)



Spark 应用程序提交到 Yarn 环境中执行的时候，一般会有两种部署执行的方式：Client 和 Cluster。两种模式主要**区别在于：Driver 程序的运行节点位置**

##### Yarn Client模式

**Client 模式将用于监控和调度的 Driver 模块在客户端执行，而不是在 Yarn 中，所以一 般用于测试。** 

➢ Driver 在任务提交的本地机器上运行 

➢ Driver 启动后会和 ResourceManager 通讯申请启动 ApplicationMaster 

➢ ResourceManager 分配 container，在合适的 NodeManager 上启动 ApplicationMaster，负责向 ResourceManager 申请 Executor 内存 

➢ ResourceManager 接到 ApplicationMaster 的资源申请后会分配 container，然后 ApplicationMaster 在资源分配指定的 NodeManager 上启动 Executor 进程

➢ Executor 进程启动后会向 Driver 反向注册，Executor 全部注册完成后 Driver 开始执行 main 函数 

➢ 之后执行到 Action 算子时，触发一个 Job，并根据宽依赖开始划分 stage，每个 stage 生成对应的 TaskSet，之后将 task 分发到各个 Executor 上执行。 



##### Yarn Cluster模式

**Cluster 模式将用于监控和调度的 Driver 模块启动在 Yarn 集群资源中执行。一般应用于 实际生产环境。** 

➢ 在 YARN Cluster 模式下，任务提交后会和 ResourceManager 通讯申请启动 

ApplicationMaster， 

➢ 随后 ResourceManager 分配 container，在合适的 NodeManager 上启动 ApplicationMaster， 

此时的 ApplicationMaster 就是 Driver。 

➢ Driver 启动后向 ResourceManager 申请 Executor 内存，ResourceManager 接到 

ApplicationMaster 的资源申请后会分配 container，然后在合适的 NodeManager 上启动 

Executor 进程 

➢ Executor 进程启动后会向 Driver 反向注册，Executor 全部注册完成后 Driver 开始执行 

main 函数， 

➢ 之后执行到 Action 算子时，触发一个 Job，并根据宽依赖开始划分 stage，每个 stage 生 

成对应的 TaskSet，之后将 task 分发到各个 Executor 上执行